"""
 í•œê¸€ ì†ê¸€ì”¨ OCR Playground  v3.4
 ----------------------------------------------------------
 â€¢ EasyOCR / OCR.space / Google Vision 3ê°œ ì—”ì§„ì„ í•œ í™”ë©´ì—ì„œ ë¹„êµ
 â€¢ ë…¸ë€ í¬ìŠ¤íŠ¸ì‡ ìë™ ê²€ì¶œ(Adaptive HSV) + ë””ë²„ê¹…(ë§ˆìŠ¤í¬, BBox)
 â€¢ OCR.spaceâ€†API Key ì§ì ‘ ì…ë ¥, Google Vision JSON ì—…ë¡œë“œ ì§€ì›
 â€¢ ROI ì—…ìŠ¤ì¼€ì¼ë§(1Ã—~4Ã—)ë¡œ ë‚œí•´í•œ ì†ê¸€ì”¨ ê°€ë…ì„± í–¥ìƒ
 â€¢ Pillow 10 ëŒ€ì‘ ëª½í‚¤íŒ¨ì¹˜  (Image.ANTIALIAS â†’ Image.Resampling.LANCZOS)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  import  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, io, json, cv2, requests
import numpy as np
import streamlit as st
from PIL import Image
import easyocr
from google.cloud import vision
from dotenv import load_dotenv

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Pillow 10 ëŒ€ì‘ ëª½í‚¤íŒ¨ì¹˜  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not hasattr(Image, "ANTIALIAS"):  # Pillow â‰¥10
    Image.ANTIALIAS = Image.Resampling.LANCZOS

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ê¸°ë³¸ í™˜ê²½ ì„¤ì •  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
st.set_page_config(page_title="ğŸ–‹ï¸ Korean Handwriting OCR Playground", layout="centered")
st.title("ğŸ–‹ï¸ í•œê¸€ ì†ê¸€ì”¨ OCR Playground")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Sidebar  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.sidebar.header("âš™ï¸ ì„¤ì •")

# --- OCR.space Key ì…ë ¥
OCR_SPACE_API_KEY = st.sidebar.text_input(
    "OCR.space API Key", value=os.getenv("OCR_SPACE_API_KEY", ""),
    type="password", placeholder="ë¹ˆ ì¹¸ì´ë©´ í˜¸ì¶œ ì•ˆ í•¨",
)

# --- Google Vision JSON ì—…ë¡œë“œ
uploaded_json = st.sidebar.file_uploader(
    "Google Vision service-account JSON", type=["json"], accept_multiple_files=False
)
VISION_JSON_DATA = json.load(uploaded_json) if uploaded_json else None

# --- ì—…ìŠ¤ì¼€ì¼ ë°°ìœ¨
upscale_factor = st.sidebar.slider("ROI ì—…ìŠ¤ì¼€ì¼ ë°°ìœ¨", 1, 4, 1)

# --- Post-it HSV ë²”ìœ„ & í•„í„°
st.sidebar.markdown("### âœï¸ í¬ìŠ¤íŠ¸ì‡ HSV ì´ˆê¸°ê°’")
LOWER_YELLOW = (
    st.sidebar.slider("Lower Hue", 10, 35, 27),     # í¬ìŠ¤íŠ¸ì‡ ìƒ‰ìƒì— ë§ì¶¤ (ìµœì ê°’ 27)
    st.sidebar.slider("Lower Sat", 0, 255, 25),     # ì—°í•œ ë…¸ë€ìƒ‰ í¬í•¨
    st.sidebar.slider("Lower Val", 0, 255, 120),    # ë°ì€ ë…¸ë€ìƒ‰ ìœ„ì£¼
)
UPPER_YELLOW = (
    st.sidebar.slider("Upper Hue", 30, 60, 35),     # ë…¸ë€ìƒ‰ ë²”ìœ„
    st.sidebar.slider("Upper Sat", 0, 255, 255),
    st.sidebar.slider("Upper Val", 0, 255, 255),
)
MIN_AREA    = st.sidebar.number_input("ìµœì†Œ ë©´ì (px)", 1000, 500000, 8000, step=1000)  # í¬ìŠ¤íŠ¸ì‡ í¬ê¸°ì— ë§ì¶¤
MAX_AR_DIFF = st.sidebar.slider("ê°€ë¡œ/ì„¸ë¡œ ë¹„ìœ¨ í—ˆìš©í¸ì°¨(%)", 0, 100, 50)  # ì •ì‚¬ê°í˜•ì— ê°€ê¹ê²Œ

show_debug = st.sidebar.checkbox("ğŸ©º ë””ë²„ê·¸ ëª¨ë“œ (Raw JSON / ë§ˆìŠ¤í¬ ì¶œë ¥)")
st.sidebar.markdown("---\nMade with â¤ï¸ 2025")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def pil_to_bytes(pil_img: Image.Image, fmt="JPEG") -> bytes:
    buf = io.BytesIO()
    pil_img.save(buf, format=fmt)
    return buf.getvalue()

# ---------- Adaptive HSV ë§ˆìŠ¤í¬ ----------
def adaptive_inrange(hsv_img, base_low, base_up):
    """ë‹¤ì–‘í•œ HSV ë²”ìœ„ë¥¼ ì‹œë„í•˜ì—¬ ìµœì ì˜ ë§ˆìŠ¤í¬ í™•ë³´"""
    low = list(base_low)
    up  = list(base_up)
    
    # 1ë‹¨ê³„: Saturation í•˜í•œì„ ë‹¨ê³„ì ìœ¼ë¡œ ë‚®ì¶¤
    for sat in (low[1], 40, 25, 10, 5):
        low[1] = sat
        mask = cv2.inRange(
            hsv_img, np.array(low, np.uint8), np.array(up, np.uint8)
        )
        if cv2.countNonZero(mask) > 2000:   # 2k í”½ì…€ ì´ìƒì´ë©´ ì„±ê³µ
            return mask
    
    # 2ë‹¨ê³„: Value í•˜í•œë„ ë‚®ì¶°ë³´ê¸°
    low = list(base_low)
    for val in (30, 20, 10):
        for sat in (5, 3, 1):
            low[1] = sat
            low[2] = val
            mask = cv2.inRange(
                hsv_img, np.array(low, np.uint8), np.array(up, np.uint8)
            )
            if cv2.countNonZero(mask) > 1000:   # 1k í”½ì…€ ì´ìƒì´ë©´ ì„±ê³µ
                return mask
    
    return mask  # ë§ˆì§€ë§‰ ê²°ê³¼ ë°˜í™˜

# ---------- Post-it íƒì§€ ----------
def find_postit(pil_img: Image.Image, debug=False):
    """ë…¸ë€ í¬ìŠ¤íŠ¸ì‡ ROI ë°˜í™˜, debug=Trueë©´ (roi, mask, bbox_img)"""
    bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
    hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)

    mask = adaptive_inrange(hsv, LOWER_YELLOW, UPPER_YELLOW)

    # ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•œ ëª¨í´ë¡œì§€ ì—°ì‚°
    kernel = np.ones((3, 3), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)  # ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°
    kernel = np.ones((7, 7), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=3)  # ë” ê°•í•œ êµ¬ë© ë©”ìš°ê¸°
    
    # ì¶”ê°€: ë” í° ì»¤ë„ë¡œ í•œ ë²ˆ ë” ì •ë¦¬
    kernel_large = np.ones((10, 10), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel_large, iterations=1)

    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not cnts:
        if debug:
            st.warning(f"ìœ¤ê³½ì„ ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë§ˆìŠ¤í¬ í”½ì…€ ìˆ˜: {cv2.countNonZero(mask)}")
        return (None, mask, None) if debug else None

    best, best_score = None, 0
    candidates = []
    
    for i, c in enumerate(cnts):
        # ê¸°ë³¸ ë°”ìš´ë”© ë°•ìŠ¤
        x, y, cw, ch = cv2.boundingRect(c)
        area = cw * ch
        ar_diff = abs(cw - ch) / max(cw, ch) * 100
        
        # ìœ¤ê³½ì„  approximationìœ¼ë¡œ ì‚¬ê°í˜•ì„± ê²€ì‚¬
        epsilon = 0.02 * cv2.arcLength(c, True)
        approx = cv2.approxPolyDP(c, epsilon, True)
        rect_score = len(approx)  # 4ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì‚¬ê°í˜•
        
        # ì»¨ë²¡ìŠ¤ í—ê³¼ì˜ ë¹„êµë¡œ ëª¨ì–‘ ê²€ì‚¬
        hull = cv2.convexHull(c)
        hull_area = cv2.contourArea(hull)
        solidity = area / hull_area if hull_area > 0 else 0
        
        # ì´ë¯¸ì§€ ì¤‘ì•™ì—ì„œì˜ ê±°ë¦¬ (í¬ìŠ¤íŠ¸ì‡ì€ ë³´í†µ ì¤‘ì•™ ê·¼ì²˜ì— ìˆìŒ)
        img_h, img_w = pil_img.size[1], pil_img.size[0]
        center_x, center_y = x + cw//2, y + ch//2
        center_dist = np.sqrt((center_x - img_w//2)**2 + (center_y - img_h//2)**2)
        normalized_center_dist = center_dist / np.sqrt(img_w**2 + img_h**2)
        
        # ë§ˆìŠ¤í¬ì—ì„œ ì‹¤ì œ ì±„ì›Œì§„ ë¹„ìœ¨
        mask_roi = mask[y:y+ch, x:x+cw]
        fill_ratio = cv2.countNonZero(mask_roi) / (cw * ch) if cw * ch > 0 else 0
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (í¬ìŠ¤íŠ¸ì‡ íŠ¹ì§•ì— ë§ì¶° ê°€ì¤‘ì¹˜ ì¡°ì •)
        score = 0
        if area >= MIN_AREA:
            # 1. ê¸°ë³¸ ë©´ì  ì ìˆ˜ (í¬ê¸°ê°€ ì ë‹¹í•´ì•¼ í•¨)
            ideal_area = 50000  # ëŒ€ëµì ì¸ í¬ìŠ¤íŠ¸ì‡ ì´ìƒì  í¬ê¸°
            area_score = max(0, 100 - abs(area - ideal_area) / ideal_area * 50)
            score += area_score
            
            # 2. ì •ì‚¬ê°í˜• ì ìˆ˜ (ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œ)
            square_score = max(0, 80 - ar_diff * 2)  # ê°€ì¤‘ì¹˜ ì¦ê°€
            score += square_score
            
            # 3. ì‚¬ê°í˜• ëª¨ì–‘ ì ìˆ˜
            rect_shape_score = min(40, (8 - abs(rect_score - 4)) * 10)  # ê°€ì¤‘ì¹˜ ì¦ê°€
            score += rect_shape_score
            
            # 4. ì±„ì›€ ë¹„ìœ¨ ì ìˆ˜ (ë§¤ìš° ì¤‘ìš”)
            fill_score = fill_ratio * 60  # ê°€ì¤‘ì¹˜ ì¦ê°€
            score += fill_score
            
            # 5. ë³¼ë¡ë„ ì ìˆ˜
            solidity_score = solidity * 25
            score += solidity_score
            
            # 6. ì¤‘ì•™ ìœ„ì¹˜ ë³´ë„ˆìŠ¤ (í¬ìŠ¤íŠ¸ì‡ì€ ë³´í†µ ì¤‘ì•™ ê·¼ì²˜)
            center_score = max(0, 15 - normalized_center_dist * 30)
            score += center_score
            
            # 7. í¬ê¸° ë¹„ìœ¨ ë³´ë„ˆìŠ¤ (ì´ë¯¸ì§€ ëŒ€ë¹„ ì ë‹¹í•œ í¬ê¸°)
            img_area = img_w * img_h
            size_ratio = area / img_area
            if 0.02 < size_ratio < 0.3:  # ì´ë¯¸ì§€ì˜ 2%~30% í¬ê¸°ê°€ ì ë‹¹
                score += 20
        
        candidates.append({
            'bbox': (x, y, cw, ch),
            'area': area,
            'ar_diff': ar_diff,
            'rect_score': rect_score,
            'solidity': solidity,
            'center_dist': normalized_center_dist,
            'fill_ratio': fill_ratio if 'fill_ratio' in locals() else 0,
            'total_score': score,
            'valid': area >= MIN_AREA and ar_diff <= MAX_AR_DIFF
        })
        
        if score > best_score:
            best_score = score
            best = (x, y, cw, ch)

    # í›„ë³´ë“¤ì„ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
    candidates.sort(key=lambda x: x['total_score'], reverse=True)

    if debug:
        st.write(f"ì´ {len(candidates)}ê°œ ìœ¤ê³½ì„  ë°œê²¬ (ì ìˆ˜ìˆœ ì •ë ¬):")
        for i, cand in enumerate(candidates[:5]):  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
            st.write(f"  {i+1}: ë©´ì ={cand['area']}, ë¹„ìœ¨ì°¨ì´={cand['ar_diff']:.1f}%, "
                    f"ì‚¬ê°í˜•ì ìˆ˜={cand['rect_score']}, ë³¼ë¡ë„={cand['solidity']:.2f}, "
                    f"ì¤‘ì‹¬ê±°ë¦¬={cand['center_dist']:.2f}, ì±„ì›€ë¹„ìœ¨={cand['fill_ratio']:.2f}, "
                    f"**ì´ì ={cand['total_score']:.1f}**, ìœ íš¨={cand['valid']}")

    if best is None or best_score < 50:  # ìµœì†Œ ì ìˆ˜ ê¸°ì¤€
        if debug:
            st.warning(f"ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í¬ìŠ¤íŠ¸ì‡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ìµœê³ ì ìˆ˜: {best_score:.1f})")
        return (None, mask, None) if debug else None

    x, y, cw, ch = best
    roi = pil_img.crop((x, y, x + cw, y + ch))

    if debug:
        bbox_img = bgr.copy()
        # ëª¨ë“  í›„ë³´ë¥¼ ì—°í•œ ìƒ‰ìœ¼ë¡œ í‘œì‹œ
        for i, cand in enumerate(candidates[:3]):
            cx, cy, ccw, cch = cand['bbox']
            color = (100, 100, 255) if i > 0 else (0, 0, 255)  # ìµœê³ ì ìˆ˜ëŠ” ë¹¨ê°„ìƒ‰, ë‚˜ë¨¸ì§€ëŠ” ì—°í•œ íŒŒë€ìƒ‰
            thickness = 3 if i == 0 else 1
            cv2.rectangle(bbox_img, (cx, cy), (cx + ccw, cy + cch), color, thickness)
            cv2.putText(bbox_img, f"{cand['total_score']:.0f}", 
                       (cx, cy-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        bbox_img = cv2.cvtColor(bbox_img, cv2.COLOR_BGR2RGB)
        st.success(f"í¬ìŠ¤íŠ¸ì‡ ê²€ì¶œ ì„±ê³µ! ì ìˆ˜: {best_score:.1f}, í¬ê¸°: {cw}Ã—{ch}")
        return roi, mask, bbox_img
    return roi

# ---------- ROI ì—…ìŠ¤ì¼€ì¼ ----------
def upscale(pil_img: Image.Image, factor: int):
    if factor == 1:
        return pil_img
    w, h = pil_img.size
    return pil_img.resize((w * factor, h * factor), Image.ANTIALIAS)

# ---------- EasyOCR ----------
@st.cache_resource(show_spinner=False)
def get_easyocr_reader():
    return easyocr.Reader(['ko'], gpu=False)

def run_easyocr(pil_img: Image.Image):
    reader = get_easyocr_reader()
    result = reader.readtext(np.array(pil_img))
    return [
        {"text": t, "conf": f"{c * 100:.1f}%", "bbox": b}
        for b, t, c in result
    ]

# ---------- OCR.space ----------
def run_ocrspace(pil_img: Image.Image, api_key: str):
    if not api_key:
        st.warning("ğŸ”‘ OCR.space API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # (ìš©ëŸ‰ ì´ˆê³¼ ë°©ì§€) 2048 px ì´í•˜ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
    max_side = 2048
    w, h = pil_img.size
    if max(w, h) > max_side:
        ratio = max_side / max(w, h)
        pil_img = pil_img.resize((int(w * ratio), int(h * ratio)), Image.ANTIALIAS)

    img_bytes = pil_to_bytes(pil_img, "JPEG")
    if len(img_bytes) > 1.5 * 1024 * 1024:  # 1.5 MB ì´ìƒ â†’ í’ˆì§ˆ 75
        buf = io.BytesIO()
        pil_img.save(buf, format="JPEG", quality=75, optimize=True)
        img_bytes = buf.getvalue()

    payload = {
        "apikey": api_key,
        "language": "kor",
        "OCREngine": 2,             # 2ê°€ ë¹„êµì  ì•ˆì •
        "scale": True,
        "detectOrientation": True,
        "isOverlayRequired": False,
        "filetype": "JPG",
    }
    files = {"file": ("image.jpg", img_bytes, "image/jpeg")}

    try:
        res = requests.post(
            "https://api.ocr.space/parse/image",
            data=payload, files=files, timeout=90
        )
        data = res.json()
    except Exception as e:
        st.error(f"OCR.space í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []

    if show_debug:
        st.expander("ğŸ” OCR.space Raw JSON").json(data)

    if data.get("IsErroredOnProcessing"):
        err = data.get("ErrorMessage", "Unknown error")
        st.error(f"OCR.space Error: {err}")
        return []

    parsed = data.get("ParsedResults", [])
    lines = []
    for pr in parsed:
        for line in pr.get("ParsedText", "").splitlines():
            t = line.strip()
            if t:
                lines.append({"text": t, "conf": "-", "bbox": None})
    return lines

# ---------- Google Vision ----------
@st.cache_resource(show_spinner=False)
def get_vision_client(json_dict):
    from google.oauth2 import service_account
    creds = service_account.Credentials.from_service_account_info(
        json_dict, scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )
    return vision.ImageAnnotatorClient(credentials=creds)

def run_vision(pil_img: Image.Image, json_dict):
    if json_dict is None:
        st.warning("ğŸ”‘ Google Vision JSONì´ ì—†ìŠµë‹ˆë‹¤.")
        return []

    client = get_vision_client(json_dict)
    img = vision.Image(content=pil_to_bytes(pil_img, "PNG"))
    resp = client.document_text_detection(image=img)

    if show_debug:
        st.expander("ğŸ” Google Vision Raw").json(
            resp._pb.SerializeToString().hex()[:2000] + "..."
        )

    lines = []
    for page in resp.full_text_annotation.pages:
        for block in page.blocks:
            for para in block.paragraphs:
                txt = "".join([s.text for w in para.words for s in w.symbols])
                conf = f"{para.confidence * 100:.1f}%"
                lines.append({"text": txt, "conf": conf, "bbox": None})
    return lines

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ë©”ì¸ UI ì˜ì—­  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("**â‘  ì—”ì§„ ì„ íƒ â†’ â‘¡ ì´ë¯¸ì§€ ì—…ë¡œë“œ â†’ â‘¢ [ğŸ” OCR ì‹¤í–‰]** ìˆœì„œë¡œ ì´ìš©í•˜ì„¸ìš”.")

engine     = st.selectbox("ì—”ì§„", ["EasyOCR", "OCR.space", "Google Vision"])
use_postit = st.checkbox("í¬ìŠ¤íŠ¸ì‡ ìë™ ê²€ì¶œ", value=True)

uploaded = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ (jpg / png)", ["jpg", "jpeg", "png"])

if uploaded:
    pil_img = Image.open(uploaded).convert("RGB")

    # â”€â”€â”€â”€â”€ í¬ìŠ¤íŠ¸ì‡ íƒì§€ â”€â”€â”€â”€â”€
    roi = None
    if use_postit:
        res = find_postit(pil_img, debug=show_debug)
        if isinstance(res, tuple):
            roi, mask_img, bbox_img = res
            if show_debug and mask_img is not None:
                st.image(mask_img, caption="Yellow Mask", use_column_width=True)
            if show_debug and bbox_img is not None:
                st.image(bbox_img, caption="Detected BBox", use_column_width=True)
        else:
            roi = res

        if roi is None:
            st.info("í¬ìŠ¤íŠ¸ì‡ì„ ì°¾ì§€ ëª»í•´ **ì›ë³¸ ì „ì²´**ë¡œ OCRì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            target_img = pil_img
        else:
            target_img = roi
    else:
        target_img = pil_img

    # â”€â”€â”€â”€â”€ ì—…ìŠ¤ì¼€ì¼ â”€â”€â”€â”€â”€
    target_img = upscale(target_img, upscale_factor)
    st.image(target_img, caption="OCR ëŒ€ìƒ ì´ë¯¸ì§€", use_column_width=True)

    if st.button("ğŸ” OCR ì‹¤í–‰"):
        with st.spinner(f"{engine} ë¶„ì„ ì¤‘..."):
            if engine == "EasyOCR":
                lines = run_easyocr(target_img)
            elif engine == "OCR.space":
                lines = run_ocrspace(target_img, OCR_SPACE_API_KEY)
            else:
                lines = run_vision(target_img, VISION_JSON_DATA)

        if not lines:
            st.warning("í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.subheader("âœ… ì¸ì‹ ê²°ê³¼")
            for l in lines:
                st.write(f"- **{l['text']}**  ({l['conf']})")

            # EasyOCR BBox ì‹œê°í™”
            if engine == "EasyOCR":
                img_np = np.array(target_img).copy()
                for l in lines:
                    if l["bbox"] is not None:
                        pts = np.array(l["bbox"], np.int32)
                        cv2.polylines(img_np, [pts], True, (0, 255, 0), 2)
                st.image(img_np, caption="EasyOCR Bounding Boxes",
                         use_column_width=True)

